{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"graveyard_reproduction_code_all_loss_v2.ipynb","provenance":[{"file_id":"1yFuekc-dj6SilR0JIc067yvLews9yyhX","timestamp":1593323645142}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4s86EvOwok_O","colab_type":"text"},"source":["# Graveyard code Seger"]},{"cell_type":"code","metadata":{"id":"JoY2q4lqoj3O","colab_type":"code","colab":{}},"source":["############################## Training One Epoch #################################\n","def standard_train(disp_net, pose_net, loss_function, epipolar_loss, mvs_loss, optimizer, n_epochs, train_loader, val_loader, test_loader):\n","    abs_diff, abs_rel, sq_rel, rmse, rmse_log = [], [], [], [], []   \n","    loss_train, loss_val, b100loss_list = [], [], []                                             # Initialized a new b100 list for plotting intermediate loss results\n","    # loop over the total number of epochs\n","    for epoch in range(n_epochs):\n","        running_loss = 0\n","        running_loss_val = 0\n","        b100loss = 0\n","\n","        ############################ Training #################################\n","\n","        # set the network architectures to training mode\n","        disp_net.train()\n","        pose_net.train()\n","        flow_net.train()\n","        \n","        # loop through the batches\n","        for i, data in enumerate(train_loader):\n","\n","            # extract source and target image for 1 forward pass. Send to GPU\n","            tgt_img = data['target_image'].to(device)\n","            src_img_prev = data['source_image_prev'].to(device) \n","            src_img_next = data['source_image_next'].to(device) \n","\n","            # Concatenate source images\n","            src_images = [src_img_prev , src_img_next]\n","\n","            # concatenate source and target image. \n","            concat_tgt_src = [torch.cat((tgt_img, src_img), 1) for src_img in src_images] # [[B, 6, H, W], .. (2x)]        \n","\n","            ############################# DEPTH #############################\n","            # predict disparity of target image and translate to depth at four scales\n","            disparities_tgt = disp_net(tgt_img)\n","            depths_tgt = [1/disp for disp in disparities_tgt] # [[B, 1, H, W], ... (4x) ]                       \n","            \n","            # predict disparities of source images and tranlsate to depth at four scales for each src image\n","            disparities_src_prev = disp_net(src_img_prev)\n","            disparities_src_next = disp_net(src_img_next)\n","            disparities_src = [*disparities_src_prev, *disparities_src_next]\n","            depths_src = [1/disp for disp in disparities_src] # [[B, 1, H, W], ... (8x) ]\n","\n","            #############################  POSE  ##############################\n","            # predict pose focal lengths \n","            pose, focal_lengths = pose_net(tgt_img, src_images)  # [B, C=2, 6] and [B, C=2, 2]\n","                    \n","            # calculate the camera intrinsics matrix\n","            intrinsics = torch.stack([focal2intrinsics(focal_lengths[:,c,:], tgt_img) for c in range(len(src_images))], dim=1) #[B, C=2, 3, 3]        \n","\n","            ############################## FLOW ################################\n","\n","            # predict flow map between the two source images and target image. Put in a list\n","            flow = [*flow_net(concat_tgt_src[0]), *flow_net(concat_tgt_src[1])] # [[B, 2, H, W], ...x8]\n","\n","\n","            ############################## LOSS ###############################\n","            loss_pc, warped, diff = loss_function(tgt_img, src_images, depths_tgt, flow, pose, intrinsics) ### ADD 'flow' BETWEEN DEPTHS AND POSE FOR APC \n","            loss_e = epipolar_loss(intrinsics, pose, src_images, flow)                               ### comment this loss away\n","            loss_mvs = mvs_loss(intrinsics, pose, src_images, tgt_img, depths_src, depths_tgt)\n","            loss = loss_pc + 0.1 * loss_mvs# + 0.01 * loss_e\n","\n","            ########################### BACKPROP ##############################\n","            # Zero the gradients, backprop and optimization of parameters\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            ######################## CALCULATE LOSSES ######################\n","            # obtain total running loss for each epoch\n","            running_loss += loss\n","            b100loss += loss\n","\n","            ################# PRINTING AFTER 100 BATCHES ###################\n","            if i % 3 == 0 and i != 0:\n","                print('loss e =', loss_e)\n","                print('loss mvs =', loss_mvs)\n","                print('loss ap =', loss_pc)\n","                avg_100_batch_loss = float(b100loss / 100)\n","                b100loss_list.append(avg_100_batch_loss)\n","                print('batch: ', i)\n","                print('100 batch avg loss = ', avg_100_batch_loss)   \n","\n","                # Save the results and plot the training curve \n","                save_images_norm(output_directory_train, depths_tgt, tgt_img, i)\n","                plot_train_curve(b100loss_list)\n","\n","                b100loss = 0\n","\n","                ############### VALIDATE EVERY 100 BATCHES #####################\n","                abs_diff, abs_rel, sq_rel, rmse, rmse_log = validate_with_gt_during_training(test_loader, disp_net, abs_diff, abs_rel, sq_rel, rmse, rmse_log)  \n","                plot_metrics(abs_diff, abs_rel, sq_rel, rmse, rmse_log)\n","\n","                # save_models(disp_net, pose_net, flow_net, model_path_disp, epoch) ################################## COMMENT IN!\n","                print('Model_saved')\n","\n","        ############################# Validation Phase ################################\n","        disp_net.eval()\n","        pose_net.eval()\n","        flow_net.eval()\n","\n","        for i, data in enumerate(val_loader):\n","            # extract source and target image for 1 forward pass. Send to GPU\n","            tgt_img = data['target_image'].to(device)\n","            src_img_prev = data['source_image_prev'].to(device) \n","            src_img_next = data['source_image_next'].to(device) \n","            src_images = [src_img_prev , src_img_next]\n","\n","            with torch.no_grad():\n","                \n","                # Concatenate source images. First source image, followed by target\n","                src_images = [src_img_prev , src_img_next]\n","\n","                # concatenate source and target image. \n","                concat_tgt_src = [torch.cat((tgt_img, src_img), 1) for src_img in src_images] # [[B, 6, H, W], .. (2x)]       \n","\n","                ############################# DEPTH #############################\n","                # predict disparity of target image and translate to depth at four scales\n","                disparities_tgt = disp_net(tgt_img)\n","                depths_tgt = [(1/disp).unsqueeze(1) for disp in disparities_tgt] # [[B, 1, H, W], ... (4x) ]                       \n","                \n","                # predict disparities of source images and translate to depth at four scales for each src image\n","                disparities_src_prev = disp_net(src_img_prev)\n","                disparities_src_next = disp_net(src_img_next)\n","                disparities_src = [*disparities_src_prev, *disparities_src_next]\n","\n","                depths_src = [(1/disp).unsqueeze(1) for disp in disparities_src] # [[B, 1, H, W], ... (8x) ]\n","                                \n","                pose, focal_lengths = pose_net(tgt_img, src_images)  \n","\n","                # calculate the camera intrinsics matrix\n","                intrinsics = torch.stack([focal2intrinsics(focal_lengths[:,c,:], tgt_img) for c in range(len(src_images))], dim=1) #[B, C=2, 3, 3]                \n","\n","                ############################# FLOW ################################\n","\n","                # predict flow map between the two source images and target image. Put in a list\n","                flow = [*flow_net(concat_tgt_src[0]), *flow_net(concat_tgt_src[1])] # [[B, 2, H, W], ...x8]\n","\n","                ############################## LOSS ###############################\n","\n","                loss_pc, warped, diff = loss_function(tgt_img, src_images, depths_tgt, flow, pose, intrinsics) ### ADD 'flow' BETWEEN DEPTHS AND POSE FOR APC\n","                #loss_e = epipolar_loss(intrinsics, pose, src_images, flow)\n","                loss_mvs = mvs_loss(intrinsics, pose, src_images, tgt_img, depths_src, depths_tgt)\n","                loss = loss_pc + 0.1 * loss_mvs# + 0.01 * loss_e\n","                running_loss_val += loss\n","\n","        ###############################################################################\n","        \n","        # Calculate training van validation loss and plot the resulting curves\n","        running_loss_avg = running_loss/(len(train_loader.dataset)/4)\n","        running_loss_val_avg = running_loss_val/len(val_loader.dataset)\n","        loss_train.append(running_loss_avg)\n","        loss_val.append(running_loss_val_avg)\n","        plot_curve(loss_train, loss_val)\n","\n","        print('epoch:', epoch)\n","        print('Training Loss: {:.4f}'.format(running_loss/(len(train_loader.dataset)/4)))\n","        print('Validation Loss: {:.4f}'.format(running_loss_val/(len(val_loader.dataset)/1)))\n","\n","\n"," \n","        \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUvAlU0LOogY","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"zDYKIkXdOpq2","colab_type":"code","colab":{}},"source":["def adaptive_photometric_loss(tgt_img, src_imgs, depths, flows, pose, intrinsics):\n","    \"\"\" Calculate the adaptive photometric loss\n","    Args:\n","        tgt_img: target image                                     -- [B, 3, H, W]\n","        src_imgs: list of the source images (previous & next)     -- [[B, 3, H, W], [B, 3, H, W]]\n","        depths: list of depth maps of target images on 4 scales   -- [[B, 1, H, W], [B, 1, H, W], [B, 1, H, W], [B, 1, H, W]]\n","        flows: flow maps                                          -- [[B, 2, H, W]....8x] \n","        pose: 6DoF pose parameters from target to source          -- [B, C=2, 6]\n","        intrinsics: camera intrinsic matrix                       -- [B, C=2, 3, 3]\n","    Return:\n","        total adaptive photometric loss\n","    \"\"\"\n","    def one_scale_pc(depth): # [B, 1, H, W]\n","        assert(pose.size(1) == len(src_imgs))\n","       \n","        reconstruction_loss = 0\n","\n","        # retrieve depth size\n","        b, _, h, w = depth.size()\n","        downscale = tgt_img.size(2)/h\n","        \n","        tgt_img_scaled = F.interpolate(tgt_img, (h, w), mode='area') # [B, 3, H, W]\n","        src_imgs_scaled = [F.interpolate(src_img, (h, w), mode='area') for src_img in src_imgs] # [[B, 3, H, W], [B, 3, H, W]]\n","        \n","        downscale_matrix = torch.tensor([[1, 1, 1/downscale],\n","                        [1, 1, 1/downscale],\n","                        [1, 1, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n","\n","        intrinsics_scaled = intrinsics * downscale_matrix #[B, C=2, 3, 3]\n","        \n","        warped_imgs = []\n","        diff_maps = []\n","        \n","        for i, src_img in enumerate(src_imgs_scaled):\n","            current_intrinsics = intrinsics_scaled[:,i] # [B, 3, 3]\n","            current_pose = pose[:,i]\n","            \n","            # warp a source image to the target image plane -- ######### changed input from src_img to tgt_img ########\n","            projected_image, valid_points = inverse_warp(tgt_img, depth[:,0], current_pose, current_intrinsics) # [B, 3, H, W], # [B, H, W]\n","            \n","            # calculate loss\n","            ssim_loss = pytorch_ssim.SSIM(window_size = 11)\n","\n","            ##################### changed to src_img & src_img_scaled_valid\n","            src_img_scaled_valid = src_img * valid_points.unsqueeze(1).float() \n","            projected_image_valid = projected_image * valid_points.unsqueeze(1).float()\n","\n","            ####################### changed to src_img_scaled_valid ##########################\n","            ssim = ssim_loss(src_img_scaled_valid, projected_image_valid) #* valid_points.unsqueeze(1).float() # value\n","            #ssim_abs = ssim.mean()\n","\n","            ####################### changed to src_img_scaled_valid ######################\n","            diff = (src_img_scaled_valid - projected_image) * valid_points.unsqueeze(1).float() \n","            diff_abs = diff.abs().mean()\n","            \n","            #reconstruction_loss = diff_abs\n","            reconstruction_loss += 0.85 * ((1-ssim)/2) + (1-0.85) * diff_abs\n","\n","            warped_imgs.append(projected_image[0])\n","            diff_maps.append(diff[0])        \n","                        \n","        return reconstruction_loss, warped_imgs, diff_maps\n","    \n","    def one_scale_apc(local_flow, nr_src_img): \n","        loss_flow = 0      \n","         # retrieve depth size and downscale factor\n","        b, _, h, w = local_flow.size() \n","        downscale = tgt_img.size(2)/h\n","\n","        # Scale the source and target image to the size of the respective flow map\n","        src_imgs_scaled = [F.interpolate(src_img, (h, w), mode='area') for src_img in src_imgs] # [[B, 3, H, W], [B, 3, H, W]]\n","        tgt_img_scaled = F.interpolate(tgt_img, (h, w), mode='area') # [B, 3, H, W]\n","        \n","        # define downscaling matrix\n","        downscale_matrix = torch.tensor([[1, 1, 1/downscale],\n","                        [1, 1, 1/downscale],\n","                        [1, 1, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n","\n","        # Scale intrinsics matrix according to scale\n","        intrinsics_scaled = intrinsics * downscale_matrix #[B, C=2, 3, 3]\n","\n","        # Determine the current intrinsics and pose\n","        current_intrinsics = intrinsics_scaled[:,nr_src_img] # [B, 3, 3]\n","        current_pose = pose[:,nr_src_img]\n"," \n","        warped_imgs = []\n","        diff_maps = []\n","    \n","        # warp a source image to the target image plane using optical flow\n","        warped_image = flow_warp(local_flow, tgt_img) #[B,3,H,W]\n","        \n","        # calculate loss\n","        ssim_loss = pytorch_ssim.SSIM(window_size = 11)\n","\n","        # calculate ssim loss and diff\n","        ssim = ssim_loss(src_imgs_scaled[nr_src_img], warped_image) \n","        diff = src_imgs_scaled[nr_src_img] - warped_image\n","        diff_abs = diff.abs().mean()\n","        \n","        # Calculate loss \n","        loss_flow += 0.85 * ((1-ssim)/2) + (1-0.85) * diff_abs    \n","                        \n","        return loss_flow\n","\n","    warped_results, diff_results = [], []\n","    warped_results_flow, diff_results_flow = [], []\n","    total_loss_flow, total_loss_pc, apc_loss = 0,0,0\n","\n","    # Loop over the depths to obtain rigid photometric loss\n","    for i, depth in enumerate(depths):\n","        loss_pc, warped, diff = one_scale_pc(depth)                      \n","        total_loss_pc += loss_pc\n","        warped_results.append(warped)\n","        diff_results.append(diff)\n","\n","    # Loop over the flows to obtain adaptive photometric loss related to flow\n","    for idx, flow in enumerate(flows):\n","        if idx < 4:\n","            nr_src_img = 0\n","        else:\n","            nr_src_img = 1\n","        loss_flow = one_scale_apc(flow, nr_src_img)\n","        total_loss_flow += loss_flow\n","        warped_results_flow.append(warped)\n","\n","    loss_apc = min(total_loss_flow, total_loss_pc)\n","\n","    return loss_apc, warped_results, diff_results\n","\n","######################## NEW Photometric Loss ############################\n","\n","def standard_photometric_loss(tgt_img, src_imgs, depths, pose, intrinsics):\n","    \"\"\" Calculate the photometric loss related to rigid displacement (not adaptive)\n","    Args:\n","        tgt_img: target image                                     -- [B, 3, H, W]\n","        src_imgs: list of the source images (previous & next)     -- [[B, 3, H, W], [B, 3, H, W]]\n","        depths: list of depth maps of target images on 4 scales   -- [[B, 1, H, W], [B, 1, H, W], [B, 1, H, W], [B, 1, H, W]]\n","        pose: 6DoF pose parameters from target to source          -- [B, C=2, 6]\n","        intrinsics: camera intrinsic matrix                       -- [B, C=2, 3, 3]\n","    Return:\n","        total photometric loss related to rigid displacement\n","    \"\"\"\n","    def one_scale(depth): # [B, 1, H, W]\n","        assert(pose.size(1) == len(src_imgs))\n","       \n","        reconstruction_loss = 0\n","\n","        # retrieve depth size\n","        b, _, h, w = depth.size()\n","        downscale = tgt_img.size(2)/h\n","        \n","        tgt_img_scaled = F.interpolate(tgt_img, (h, w), mode='area') # [B, 3, H, W]\n","        src_imgs_scaled = [F.interpolate(src_img, (h, w), mode='area') for src_img in src_imgs] # [[B, 3, H, W], [B, 3, H, W]]\n","        \n","        downscale_matrix = torch.tensor([[1, 1, 1/downscale],\n","                        [1, 1, 1/downscale],\n","                        [1, 1, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n","\n","        intrinsics_scaled = intrinsics * downscale_matrix #[B, C=2, 3, 3]\n","        \n","        warped_imgs = []\n","        diff_maps = []\n","        \n","        for i, src_img in enumerate(src_imgs_scaled):\n","            current_intrinsics = intrinsics_scaled[:,i] # [B, 3, 3]\n","            current_pose = pose[:,i]\n","            \n","            # warp a source image to the target image plane -- ######### changed input from src_img to tgt_img ########\n","            projected_image, valid_points = inverse_warp(tgt_img, depth[:,0], current_pose, current_intrinsics) # [B, 3, H, W], # [B, H, W]\n","\n","            # calculate loss\n","            ssim_loss = pytorch_ssim.SSIM(window_size = 11)\n","\n","            ##################### changed to src_img & src_img_scaled_valid\n","            src_img_scaled_valid = src_img * valid_points.unsqueeze(1).float() \n","            projected_image_valid = projected_image * valid_points.unsqueeze(1).float()\n","\n","            ####################### changed to src_img_scaled_valid ##########################\n","            ssim = ssim_loss(src_img_scaled_valid, projected_image_valid) #* valid_points.unsqueeze(1).float() # value\n","            #ssim_abs = ssim.mean()\n","\n","            ####################### changed to src_img_scaled_valid ######################\n","            diff = (src_img_scaled_valid - projected_image) * valid_points.unsqueeze(1).float() \n","            diff_abs = diff.abs().mean()\n","            \n","            #reconstruction_loss = diff_abs\n","            reconstruction_loss += 0.85 * ((1-ssim)/2) + (1-0.85) * diff_abs\n","\n","            warped_imgs.append(projected_image[0])\n","            diff_maps.append(diff[0])        \n","                        \n","        return reconstruction_loss, warped_imgs, diff_maps\n","\n","    warped_results, diff_results = [], []\n","\n","    total_loss = 0\n","\n","    for i, depth in enumerate(depths):\n","        loss, warped, diff = one_scale(depth)                      \n","        total_loss += loss\n","        warped_results.append(warped)\n","        diff_results.append(diff)\n","\n","\n","    return total_loss, warped_results, diff_results\n","\n","######################### OLD PHOTOMETRIC LOSS ################################\n","\n","# def standard_photometric_loss(tgt_img, src_imgs, depths, pose, intrinsics):\n","#     \"\"\" Calculate the photometric loss related to rigid displacement (not adaptive)\n","#     Args:\n","#         tgt_img: target image                                     -- [B, 3, H, W]\n","#         src_imgs: list of the source images (previous & next)     -- [[B, 3, H, W], [B, 3, H, W]]\n","#         depths: list of depth maps of target images on 4 scales   -- [[B, 1, H, W], [B, 1, H, W], [B, 1, H, W], [B, 1, H, W]]\n","#         pose: 6DoF pose parameters from target to source          -- [B, C=2, 6]\n","#         intrinsics: camera intrinsic matrix                       -- [B, C=2, 3, 3]\n","#     Return:\n","#         total photometric loss related to rigid displacement\n","#     \"\"\"\n","#     def one_scale(depth): # [B, 1, H, W]\n","#         assert(pose.size(1) == len(src_imgs))\n","       \n","#         reconstruction_loss = 0\n","\n","#         # retrieve depth size\n","#         b, _, h, w = depth.size()\n","#         downscale = tgt_img.size(2)/h\n","        \n","#         tgt_img_scaled = F.interpolate(tgt_img, (h, w), mode='area') # [B, 3, H, W]\n","#         src_imgs_scaled = [F.interpolate(src_img, (h, w), mode='area') for src_img in src_imgs] # [[B, 3, H, W], [B, 3, H, W]]\n","        \n","#         downscale_matrix = torch.tensor([[1, 1, 1/downscale],\n","#                         [1, 1, 1/downscale],\n","#                         [1, 1, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n","\n","#         intrinsics_scaled = intrinsics * downscale_matrix #[B, C=2, 3, 3]\n","        \n","#         warped_imgs = []\n","#         diff_maps = []\n","        \n","#         for i, src_img in enumerate(src_imgs_scaled):\n","#             current_intrinsics = intrinsics_scaled[:,i] # [B, 3, 3]\n","#             current_pose = pose[:,i]\n","            \n","#             projected_image, valid_points = inverse_warp(src_img, depth[:,0], current_pose, current_intrinsics) # [B, 3, H, W], # [B, H, W]\n","\n","#             # calculate loss\n","#             ssim_loss = pytorch_ssim.SSIM(window_size = 11)\n","#             tgt_img_scaled_valid = tgt_img_scaled * valid_points.unsqueeze(1).float()\n","#             projected_image_valid = projected_image * valid_points.unsqueeze(1).float()\n","\n","#             ssim = ssim_loss(tgt_img_scaled_valid, projected_image_valid) #* valid_points.unsqueeze(1).float() # value\n","#             #ssim_abs = ssim.mean()\n","#             diff = (tgt_img_scaled - projected_image) * valid_points.unsqueeze(1).float()\n","#             diff_abs = diff.abs().mean()\n","            \n","#             #reconstruction_loss = diff_abs\n","#             reconstruction_loss += 0.85 * ((1-ssim)/2) + (1-0.85) * diff_abs\n","\n","#             warped_imgs.append(projected_image[0])\n","#             diff_maps.append(diff[0])        \n","                        \n","#         return reconstruction_loss, warped_imgs, diff_maps\n","\n","#     warped_results, diff_results = [], []\n","\n","#     total_loss = 0\n","\n","#     for i, depth in enumerate(depths):\n","#         loss, warped, diff = one_scale(depth)                      \n","#         total_loss += loss\n","#         warped_results.append(warped)\n","#         diff_results.append(diff)\n","\n","\n","#     return total_loss, warped_results, diff_results"],"execution_count":null,"outputs":[]}]}