{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reproduction_code_all_loss_v2.ipynb","provenance":[{"file_id":"1yFuekc-dj6SilR0JIc067yvLews9yyhX","timestamp":1593323645142}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mCK18zqUdW_i","colab_type":"text"},"source":["# Reproduction of Self-Supervised Learning with Geometric Constraints in Monocular Video\n","\n","by Philip de Rijk en Seger Tak"]},{"cell_type":"code","metadata":{"id":"soAXHcHBXct6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593637659783,"user_tz":-120,"elapsed":4516,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}},"outputId":"6baffb7c-0bad-4a1f-9358-3584ffbeb3c2"},"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":300,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5GAwcI3SdSRk","colab_type":"text"},"source":["## Import Modules"]},{"cell_type":"code","metadata":{"id":"m_zjFRfabff2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593601812955,"user_tz":-120,"elapsed":1082,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["# \n","import os\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F  \n","import torch.optim as optim\n","import torchvision\n","import skimage.transform\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from importlib import reload # example: foo = reload(foo) where foo = module\n","\n","root_dir = '/content/drive/My Drive/CV_by_DL_project/reproduction_chen/'\n","\n","import sys\n","sys.path.append(root_dir)\n","\n","# Models\n","from dispnet import DispNetS\n","from cameranet import CameraNet\n","from flownet import FlowNet\n","from PoseExpNet import PoseExpNet \n","\n","# Utility functions\n","import pytorch_ssim\n","from transforms_new_2 import ImageTransform\n","from dataloader import KittiLoader\n","from dataloader import gt_DepthLoader\n","from inverse_warp import inverse_warp, pose_vec2mat\n","from utils import to_device\n","import time"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6go9eQTKDKq0","colab_type":"text"},"source":["## Device"]},{"cell_type":"code","metadata":{"id":"nsKIdwntDFND","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593601812957,"user_tz":-120,"elapsed":576,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["# Use GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9xIM3sDDP5z","colab_type":"text"},"source":["## Directories & File Locations"]},{"cell_type":"code","metadata":{"id":"Bkdcc0MM-V1R","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593635849804,"user_tz":-120,"elapsed":1164,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["# kitti data directory\n","kitti_dir = os.path.join(root_dir, 'data/kitti/')\n","\n","# Define location of the text file containing training examples\n","eigen_train_file = os.path.join(kitti_dir, 'eigen_train_files_subset.txt')\n","eigen_val_file = os.path.join(kitti_dir, 'eigen_val_files.txt')\n","\n","# GT evaluation file locations \n","gt_depth_500_dir = os.path.join(root_dir, 'gt_test500/')\n","image_500_dir = os.path.join(root_dir, 'imagetest500/')\n","\n","gt_depth_500_val_dir = os.path.join(root_dir, 'gt_valid_500/')\n","image_500_val_dir = os.path.join(root_dir, 'imagevalidate500/')\n","\n","# directory to save model weights\n","model_path_disp = os.path.join(root_dir,'data/models_seger/disp_apc_mvs_e_last.pth')\n","model_path_pose = os.path.join(root_dir,'data/models_seger/pose_apc_mvs_e_last.pth')\n","model_path_flow = os.path.join(root_dir,'data/models_seger/flow_apc_mvs_e_last.pth')\n","\n","# directory to load pretrained models\n","model_path_disp_load = os.path.join(root_dir,'data/models_seger/disp_apc_working_last_SEGER4_last_SEGER.pth')\n","model_path_pose_load = os.path.join(root_dir,'data/models_seger/pose_apc_working_last_SEGER4_last_SEGER.pth')\n","model_path_flow_load = os.path.join(root_dir,'data/models_seger/flow_apc_working_last_SEGER4_last_SEGER.pth')\n","\n","# output directory for depth predictions\n","output_directory_train = os.path.join(root_dir,'data/output/train_full_seger')\n","output_directory_eval = os.path.join(root_dir,'data/output/val_full_seger')"],"execution_count":288,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tIEPic_ODVGr","colab_type":"text"},"source":["### Training Dataloader"]},{"cell_type":"code","metadata":{"id":"kBMOtRpDaFRG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":301},"executionInfo":{"status":"ok","timestamp":1593637752935,"user_tz":-120,"elapsed":9587,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}},"outputId":"ecde2a64-1c51-49d1-fa8a-5d68ec171438"},"source":["# Data augmentation \n","datatransform_train = ImageTransform(mode='train')\n","datatransform_val = ImageTransform(mode='val')\n","\n","# Run the dataloader to obtain the dataset \n","train_set = KittiLoader(split_file=eigen_train_file, base_dir=kitti_dir, mode='train', transform=datatransform_train)\n","\n","# define the dataloader variables, such as batch size etc.\n","train_loader = DataLoader(train_set, batch_size = 4, shuffle = True, num_workers = 2, pin_memory=True)\n","\n","# Run the dataloader to obtain the dataset \n","val_set = KittiLoader(split_file=eigen_val_file, base_dir=kitti_dir, mode='train', transform=datatransform_val)\n","\n","# define the dataloader variables, such as batch size etc.\n","val_loader = DataLoader(val_set, batch_size = 1, shuffle = False, num_workers = 2, pin_memory=True)\n","\n","data_loader = {'train': train_loader, 'val': val_loader} \n","\n","print('the training set contains', len(train_set), 'images')\n","print('the val set contains', len(val_set), 'images')"],"execution_count":301,"outputs":[{"output_type":"stream","text":["SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_28/2011_09_28_drive_0001_sync/image_02/data/0000000104.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0057_sync/image_02/data/0000000359.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0087_sync/image_02/data/0000000727.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0001_sync/image_02/data/0000000106.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_30/2011_09_30_drive_0020_sync/image_02/data/-000000001.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0079_sync/image_02/data/-000000001.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0018_sync/image_02/data/-000000001.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0032_sync/image_02/data/0000000388.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_30/2011_09_30_drive_0033_sync/image_02/data/0000001592.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0091_sync/image_02/data/-000000001.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0113_sync/image_02/data/0000000085.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_10_03/2011_10_03_drive_0042_sync/image_02/data/0000001168.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0015_sync/image_02/data/-000000001.png  - does not exist\n","SKIPPED:  /content/drive/My Drive/CV_by_DL_project/reproduction_chen/data/kitti/2011_09_26/2011_09_26_drive_0001_sync/image_02/data/-000000001.png  - does not exist\n","the training set contains 4086 images\n","the val set contains 885 images\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZBn_l4XlgNi2","colab_type":"text"},"source":["### Test Dataloader"]},{"cell_type":"code","metadata":{"id":"FuUuL4T-gUO9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593630496927,"user_tz":-120,"elapsed":6006,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}},"outputId":"e8a78cfe-401c-46ea-feca-4d0a3b349dea"},"source":["# Data transforms \n","datatransform_test = ImageTransform(mode='test')\n","\n","# Run the dataloader to obtain the dataset \n","test_set = gt_DepthLoader(gt_depth_500_dir, image_500_dir, datatransform_test)\n","\n","# define the dataloader variables, such as batch size etc.\n","test_loader = DataLoader(test_set, batch_size = 1, shuffle = False, num_workers = 2, pin_memory=True)\n","\n","print('the test set contains', len(test_set), 'images')"],"execution_count":263,"outputs":[{"output_type":"stream","text":["the test set contains 500 images\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kuglY3E4fU6z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593630496928,"user_tz":-120,"elapsed":4888,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}},"outputId":"40c7f367-66a5-41ef-887c-72439da009e9"},"source":["# Data transforms \n","datatransform_val500 = ImageTransform(mode='test')\n","\n","# Run the dataloader to obtain the dataset \n","val500_set = gt_DepthLoader(gt_depth_500_val_dir, image_500_val_dir, datatransform_val500)\n","\n","# define the dataloader variables, such as batch size etc.\n","val500_loader = DataLoader(val500_set, batch_size = 1, shuffle = False, num_workers = 2, pin_memory=True)\n","\n","print('the val set contains', len(val500_set), 'images')"],"execution_count":264,"outputs":[{"output_type":"stream","text":["the val set contains 500 images\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dEodp3Fzi7IF","colab_type":"text"},"source":["## Utility functions"]},{"cell_type":"code","metadata":{"id":"Y4Gnzm6Gf2l0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593630499378,"user_tz":-120,"elapsed":943,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["############################ SSIM import used for APC ################################\n","from torch.autograd import Variable\n","from math import exp\n","\n","def gaussian(window_size, sigma):\n","    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n","    return gauss/gauss.sum()\n","\n","def create_window(window_size, channel):\n","    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n","    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n","    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n","    return window\n","\n","def _ssim(img1, img2, window, window_size, channel, size_average = True):\n","    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n","    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n","\n","    mu1_sq = mu1.pow(2)\n","    mu2_sq = mu2.pow(2)\n","    mu1_mu2 = mu1*mu2\n","\n","    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n","    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n","    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n","\n","    C1 = 0.01**2\n","    C2 = 0.03**2\n","\n","    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n","\n","    if size_average:\n","        return ssim_map.mean()\n","    else:\n","        return ssim_map.mean(1)#.mean(1).mean(1)\n","\n","class SSIM(torch.nn.Module):\n","    def __init__(self, window_size = 11, size_average = False):\n","        super(SSIM, self).__init__()\n","        self.window_size = window_size\n","        self.size_average = size_average\n","        self.channel = 1\n","        self.window = create_window(window_size, self.channel)\n","\n","    def forward(self, img1, img2):\n","        (_, channel, _, _) = img1.size()\n","\n","        if channel == self.channel and self.window.data.type() == img1.data.type():\n","            window = self.window\n","        else:\n","            window = create_window(self.window_size, channel)\n","            \n","            if img1.is_cuda:\n","                window = window.cuda(img1.get_device())\n","            window = window.type_as(img1)\n","            \n","            self.window = window\n","            self.channel = channel\n","\n","\n","        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n","\n","def ssim(img1, img2, window_size = 11, size_average = False):\n","    (_, channel, _, _) = img1.size()\n","    window = create_window(window_size, channel)\n","    \n","    if img1.is_cuda:\n","        window = window.cuda(img1.get_device())\n","    window = window.type_as(img1)\n","    \n","    return _ssim(img1, img2, window, window_size, channel, size_average)"],"execution_count":265,"outputs":[]},{"cell_type":"code","metadata":{"id":"rp8e48IMi6M2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593635968316,"user_tz":-120,"elapsed":1138,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["def make_grid(depth_or_flow_pred_1d):\n","    # alternative to the set_id_grid from inverse_warp.py of Clement Pinard\n","    \"\"\" obtain pixel coordinates of an image.\n","    args: \n","        depth_or_flow_pred_1d: squeezed predicted depth or flow map. In case of flow map, only select 1 channel -- [B, C, H, W]\n","    return:\n","        pixel_coords: pixel coordinates, where each coordinate is denoted [u,v,1] -- [1, 3, H, W]\n","    \"\"\"\n","    b, _, h, w = depth_or_flow_pred_1d.size()\n","    i_range = torch.arange(0, h).view(1, h, 1).expand(1,h,w).type_as(depth_or_flow_pred_1d)  # [1, H, W]\n","    j_range = torch.arange(0, w).view(1, 1, w).expand(1,h,w).type_as(depth_or_flow_pred_1d)  # [1, H, W]\n","    ones = torch.ones(1,h,w).type_as(depth_or_flow_pred_1d)\n","\n","    pixel_coords = torch.stack((j_range, i_range, ones), dim=1)  # [1, 3, H, W]\n","\n","    return pixel_coords\n","\n","def pixel2cam_v2(depth, intrinsics_inv):\n","    # alternative to the pixel2cam from inverse_warp.py of Clement Pinard. \n","    \"\"\"Transform coordinates in the pixel frame to the camera frame.\n","    Args:\n","        depth: depth maps -- [B, 1, H, W]\n","        intrinsics_inv: intrinsics_inv matrix for each element of batch -- [B, 3, 3]\n","    Returns:\n","        array of (u,v,1) cam coordinates -- [B, 3, H, W]\n","    \"\"\"\n","    b, c, h, w = depth.size()\n","    pixel_coords = make_grid(depth)\n","    current_pixel_coords = pixel_coords[:,:,:h,:w].expand(b,3,h,w).reshape(b, 3, -1)  # [B, 3, H*W]\n","    cam_coords = (intrinsics_inv @ current_pixel_coords).reshape(b, 3, h, w)\n","    return cam_coords * depth\n","\n","def scale_intrinsics(current_scale_img, normal_scale_img, intrinsics):\n","    \"\"\" scale the intrinsics matrix with image scale\n","    \"\"\"\n","    b, _, h, w = current_scale_img.size()\n","    downscale = normal_scale_img.size(2)/h\n","    \n","    downscale_matrix = torch.tensor([[1, 1, 1/downscale],\n","                    [1, 1, 1/downscale],\n","                    [1, 1, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n","\n","    intrinsics_scaled = intrinsics * downscale_matrix #[B, C=2, 3, 3]\n","\n","    return intrinsics_scaled\n","\n","def vec2skew_symmetric(vec):\n","    \"\"\" Convert vector to skew symmetric matrix\n","    Args: \n","        vec = vector of length 3 - [B, 3]\n","    return:\n","        skew_symmetric_mat = skew symmetric matrix representation of vector [B, 3, 3]\n","    \"\"\"\n","    assert(vec.size()[1] == 3)\n","    assert(vec.ndim == 2)\n","\n","    # create zero tensor with right shape to input into skew symmetric matrix\n","    zeros = vec[:,2]*0\n","    b, _ = vec.size()   # extract batch size\n","\n","    # define skew symmetric matrix\n","    skew_symmetric_mat = torch.stack([zeros, -vec[:, 2], vec[:, 1],\n","                    vec[:, 2], zeros,   -vec[:, 0],\n","                    -vec[:, 1], vec[:, 0], zeros], dim=1).reshape(b, 3, 3)\n","    return skew_symmetric_mat\n","\n","def optical_flow_displacement(src2tgt_flow, src_img):\n","    \"\"\" Warp a source image pixels to the target image plane based on flow field\n","    Args: \n","        src2tgt_flow = source to target image flow -- [B, 2, H, W]\n","        src_img = source image -- [B, 3, H, W]\n","    Return:\n","        pixel_coords_tgt = pixels in target image corresponding to pixels in source image [B, 2, H, W]\n","        valid_points = valid points of the warped image [B, H, W]\n","    \"\"\"\n","    b, _, h, w = src2tgt_flow.size()\n","    \n","    # obtain non-homogeneous pixel coordinates of the source image\n","    pixel_coords_src = make_grid(src2tgt_flow)[:, :2] # [1, 2, H, W]\n","\n","    # Obtain pixels in the target image corresponding to pixels in source image via optical flow.\n","    pixel_coords_tgt = pixel_coords_src.repeat(b,1,1,1) + src2tgt_flow # [B, 2, H, W]\n","\n","    # # With normalizer\n","    # normalizer = torch.tensor([(2./w),(2./h)]).repeat(b,h,w,1).permute(0,3,1,2).float().to(device) \n","    # pixel_coords_tgt = (pixel_coords_tgt/normalizer).permute(0,2,3,1) # [B, H, W, 2]\n","\n","    # without normalizer\n","    pixel_coords_tgt = pixel_coords_tgt.permute(0,2,3,1) # [B, H, W, 2]\n","\n","    # return the valid points of the sampler. Check whether this is required! ############################################\n","    valid_points = pixel_coords_tgt.abs().max(dim=-1)[0] <= 1 #[b, h, w]\n","\n","    return pixel_coords_tgt.permute(0,3,1,2), valid_points\n","\n","def flow_warp(src2tgt_flow, tgt_img):\n","    \"\"\" warp a target image to the source image plane based on flow field\n","    Args: \n","        src2tgt_flow = source to target image flow -- [B, 2, H, W]\n","        tgt_img = target image from which pixels will be sampled-- [B, 3, H, W]\n","    Return:\n","        warped_img =  [B, 3, H, W]\n","      \n","    \"\"\"\n","    # obtain the valid target image pixels corresponding to the source image pixels via flow\n","    pixel_coords_tgt, valid_points = optical_flow_displacement(src2tgt_flow, tgt_img)  # [B, 2, H, W], [B,H,W]\n","\n","    #obtain valid pixels of the target images from which we will sample.\n","    # tgt_img = tgt_img.permute(0,3,1,2) * valid_points.unsqueeze(3) # [B,H,W,2]\n","\n","    ## perform grid sampling on the source image using transformed source to target pixel coordinates. Requires [b,c,h,w], [b,h,w,coordinates]\n","    warped_img = F.grid_sample(tgt_img, (pixel_coords_tgt.permute(0,2,3,1)), mode='bilinear', padding_mode='zeros', align_corners=True) \n","\n","    return warped_img\n","\n","def focal2intrinsics(focal_lengths, img):\n","    \"\"\" Create intrinsics matrix K\n","    Args:\n","        focal_lengths = focal lengths along the two optical axes fax and fay -- [B, 2] \n","        img = image of resolution H × W -- [B, 3, H, W]\n","    Returns:\n","        Camera Intrinsic Matrix K -- [B, 3, 3]\n","    \"\"\"\n","    b, _, h, w = img.size()\n","    \n","    # Retrieve individual focal lengths\n","    fax = focal_lengths[:,0]\n","    fay = focal_lengths[:,1]\n","  \n","    # create tensors from ints with same dimenstions as batch size\n","    zeros = fax*0\n","    ones = zeros+1\n","    h = ones*h\n","    w = ones*w\n","\n","    # create the camera intrinsics matrix\n","    K = torch.stack([fax,   zeros, w/2,\n","                     zeros, fay,   h/2,\n","                     zeros, zeros, ones], dim=1).reshape(b, 3, 3)\n","\n","    return K\n","\n","def save_images(output_directory_train, depths_tgt, tgt_img, i):\n","    \"\"\" save the output and target images during training, validation and testing\n","    \"\"\"\n","    depth_to_img = skimage.transform.resize(depths_tgt[0][0,:,:,:].squeeze().cpu().detach(), [375, 1242], mode='constant')\n","    tgt_to_img = skimage.transform.resize(tgt_img[0,:,:,:].squeeze().permute(1, 2, 0).cpu().detach(), [375, 1242], mode='constant')\n","    plt.imsave(os.path.join(output_directory_train, str(i)+ '_apc-mvs_e_traindepth_SEGER.png'), depth_to_img, cmap='plasma') \n","    plt.imsave(os.path.join(output_directory_train, str(i)+ '_apc-mvs_e_trainimage_SEGER.png'), tgt_to_img)\n","\n","def save_images_norm(output_directory_train, depths_tgt, tgt_img, i):\n","    \"\"\" save the normalised output and target images during training, validation and testing\n","    \"\"\" \n","    # Get disparity output\n","    disp_tgt = 1/depths_tgt[0]\n","\n","    # Make correction on the normalized disparity and depth output \n","    disp_tgt *= 256\n","    depths_tgt[0]*=256\n","\n","    # transform and save image\n","    depth_to_img = depths_tgt[0][0].squeeze().cpu().detach().numpy().astype(np.uint16)\n","    depth_to_img = skimage.transform.resize(depth_to_img, [352, 1216], mode='constant') \n","    disp_to_img = skimage.transform.resize(disp_tgt[0,:,:,:].squeeze().cpu().detach(), [375, 1242], mode='constant')\n","    plt.imsave(os.path.join(output_directory_train, str(i)+ '_apc-mvs_e_traindepth_SEGER.png'), depth_to_img)\n","    tgt_to_img = skimage.transform.resize(tgt_img[0].abs().squeeze().permute(1, 2, 0).cpu().detach(), [375, 1242], mode='constant') \n","    plt.imsave(os.path.join(output_directory_train, str(i)+ '_apc-mvs_e_traindisp_SEGER.png'), disp_to_img, cmap='plasma') \n","    plt.imsave((os.path.join(output_directory_train, str(i)+ '_apc-mvs_e_trainimage_SEGER.png')), tgt_to_img)\n","\n","def save_models(disp_net, pose_net, flow_net, model_path_disp, epoch):\n","    \"\"\" save the full networks\n","    \"\"\"\n","    torch.save(disp_net, model_path_disp[:-4] + str(epoch) + '_last_SEGER.pth')\n","    torch.save(pose_net, model_path_pose[:-4] + str(epoch) + '_last_SEGER.pth')\n","    torch.save(flow_net, model_path_pose[:-4] + str(epoch) + '_last_SEGER.pth')\n","\n","def plot_train_curve(b100loss):\n","    \"\"\" Plot the training curves every x number of batches during training\n","    \"\"\"\n","    print()\n","    plt.plot(b100loss)\n","    plt.xlabel('100 batches')\n","    plt.ylabel('training loss')\n","    plt.legend(['training curve'])\n","    plt.grid(True)\n","    plt.show() \n","\n","def plot_curve(loss_train, loss_val):\n","    \"\"\" Plot the training and validation curve after every epoch\n","    \"\"\"\n","    print()\n","    plt.plot(loss_train)\n","    plt.plot(loss_val)\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(['training loss', 'validation loss'])\n","    plt.grid(True)\n","    plt.show()\n","\n","def plot_metrics(abs_diff, abs_rel, sq_rel, rmse, rmse_log):\n","    \"\"\" Plot the metrics for testing\n","    \"\"\"\n","    print()     \n","    plt.plot(abs_diff)\n","    plt.plot(abs_rel)\n","    plt.plot(rmse)\n","    plt.plot(rmse_log)\n","    plt.xlabel('batch')\n","    plt.ylabel('error')\n","    plt.legend(['abs_diff', 'abs_rel', 'rmse', 'rmse_log'])\n","    plt.grid(True)\n","    plt.show()\n","\n","    print()     \n","    plt.plot(sq_rel)\n","    plt.xlabel('batch')\n","    plt.ylabel('error')\n","    plt.legend(['sq_rel'])\n","    plt.grid(True)\n","    plt.show()\n"],"execution_count":292,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RgWZ-30KeTSa","colab_type":"text"},"source":["## Load Models"]},{"cell_type":"code","metadata":{"id":"UfroF8BcIC8V","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593636377488,"user_tz":-120,"elapsed":2288,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["disp_net = DispNetS().to(device)\n","pose_net = CameraNet(nb_ref_imgs=2).to(device)\n","flow_net = FlowNet(6, 0.1).to(device)\n","disp_net.init_weights()\n","pose_net.init_weights()\n","flow_net.init_weight()"],"execution_count":295,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZ6hMtVAh5pI","colab_type":"text"},"source":["### pretrained dispnet"]},{"cell_type":"code","metadata":{"id":"37L4F6b13gY_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593636382168,"user_tz":-120,"elapsed":3753,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}},"outputId":"4d1f6d0f-0d7b-4df3-ff45-e77e838ead52"},"source":["pretrained_disp_path = os.path.join(root_dir, 'models/pretrained_models/dispnet_model_best.pth.tar')\n","weights = torch.load(pretrained_disp_path)\n","disp_net.load_state_dict(weights['state_dict'])  "],"execution_count":296,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":296}]},{"cell_type":"markdown","metadata":{"id":"1mqBrCqwh993","colab_type":"text"},"source":["## Optimizer"]},{"cell_type":"code","metadata":{"id":"iFpYcOvIB1Uf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593636382169,"user_tz":-120,"elapsed":1566,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["lr = 2E-4\n","b1 = 0.9\n","b2 = 0.999\n","optim_params = [\n","        {'params': disp_net.parameters(), 'lr': lr},\n","        {'params': pose_net.parameters(), 'lr': lr},\n","        {'params': flow_net.parameters(), 'lr': lr}\n","    ]\n","optimizer = optim.Adam(optim_params, betas=(b1, b2), eps=1e-08)"],"execution_count":297,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cl3-5stZFNHB","colab_type":"text"},"source":["## Losses"]},{"cell_type":"markdown","metadata":{"id":"EbAzTxjP6r75","colab_type":"text"},"source":["### Photometric losses"]},{"cell_type":"code","metadata":{"id":"q7cZw-ci6qyl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593633280222,"user_tz":-120,"elapsed":1219,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["def adaptive_photometric_loss(tgt_img, src_imgs, depths, flows, pose, intrinsics):\n","    \"\"\" Calculate the adaptive photometric loss\n","    Args:\n","        tgt_img: target image                                     -- [B, 3, H, W]\n","        src_imgs: list of the source images (previous & next)     -- [[B, 3, H, W], [B, 3, H, W]]\n","        depths: list of depth maps of source images on 4 scales   -- [[B, 1, H, W]....8x] \n","        flows: flow maps on 4 scales                              -- [[B, 2, H, W]....8x] \n","        pose: 6DoF pose parameters from target to source          -- [B, C=2, 6]\n","        intrinsics: camera intrinsic matrix                       -- [B, C=2, 3, 3]\n","    Return:\n","        total adaptive photometric loss\n","    \"\"\"\n","    def one_scale_apc(depth, nr_src_img, i): # [B, 1, H, W]\n","        assert(pose.size(1) == len(src_imgs))\n","        reconstruction_loss = 0\n","        loss_flow = 0\n","\n","        # retrieve depthmap size\n","        b, _, h, w = depth.size() \n","\n","        # Scale the source and target image to the size of the respective depth map\n","        tgt_img_scaled = F.interpolate(tgt_img, (h, w), mode='area') # [B, 3, H, W]\n","        src_imgs_scaled = [F.interpolate(src_img, (h, w), mode='area') for src_img in src_imgs] # [[B, 3, H, W], [B, 3, H, W]]\n","            \n","        # Scale intrinsics matrix according to scale\n","        intrinsics_scaled = scale_intrinsics(depth, tgt_img, intrinsics) #[B, C=2, 3, 3]\n","        \n","        # obtain the current intrinsics matrix, pose matrix and flow map.\n","        current_intrinsics = intrinsics_scaled[:,nr_src_img] # [B, 3, 3]\n","        current_pose = pose[:,nr_src_img]\n","        current_flow = flows[i]\n","            \n","        # Inverse warp a source image to the target image plane. \n","        projected_image, valid_points = inverse_warp(tgt_img, depth[:,0], current_pose, current_intrinsics) # [B, 3, H, W], # [B, H, W]\n","\n","        # warp a source image to the target image plane using optical flow\n","        warped_image_flow = flow_warp(current_flow, tgt_img_scaled) #[B,3,H,W]\n","            \n","        # Initialization of SSIM loss\n","        ssim_loss = SSIM()\n","\n","        # Calculating the L1 loss of both the rigid and optical flow related displacement\n","        diff_rigid = src_imgs_scaled[nr_src_img] - projected_image\n","        diff_rigid_abs = diff_rigid.abs().mean(1)\n","        diff_flow = src_imgs_scaled[nr_src_img] - warped_image_flow\n","        diff_flow_abs = diff_flow.abs().mean(1)\n","\n","        # calculate SSIM loss for the rigid displacement of pixels\n","        ssim_rigid = ssim_loss(src_imgs_scaled[nr_src_img], projected_image) \n","        reconstruction_loss = (0.85 * ((1-ssim_rigid)/2) + (1-0.85) * diff_rigid_abs)\n","\n","        # Calculate SSIM loss for secondary displacement of pixels related to flow\n","        ssim_flow = ssim_loss(src_imgs_scaled[nr_src_img], warped_image_flow) \n","        loss_flow = (0.85 * ((1-ssim_flow)/2) + (1-0.85) * diff_flow_abs)\n","\n","        # calculate the minimum loss value for pixel (height, width)\n","        loss_apc = torch.min(reconstruction_loss, loss_flow)\n","            \n","        # obtain total loss value for one scale. Mean or sum?!\n","        loss_apc_batch = loss_apc.mean()\n","        return loss_apc_batch\n","\n","    total_apc_loss = 0\n","    # Loop over the depths to obtain rigid photometric loss\n","    for i, depth in enumerate(depths):\n","        if i < 4:\n","            nr_src_img = 0\n","        else:\n","            nr_src_img = 1\n","        loss_apc = one_scale_apc(depth, nr_src_img, i) # [H, W]\n","        total_apc_loss += loss_apc                    \n","            \n","    # remove!\n","    warped_results = 0\n","    diff_results = 0   \n","\n","    return total_apc_loss, warped_results, diff_results\n","\n","def standard_photometric_loss(tgt_img, src_imgs, depths, pose, intrinsics):\n","    \"\"\" Calculate the photometric loss related to rigid displacement (not adaptive)\n","    Args:\n","        tgt_img: target image                                     -- [B, 3, H, W]\n","        src_imgs: list of the source images (previous & next)     -- [[B, 3, H, W], [B, 3, H, W]]\n","        depths: list of depth maps of target images on 4 scales   -- [[B, 1, H, W], [B, 1, H, W], [B, 1, H, W], [B, 1, H, W]]\n","        pose: 6DoF pose parameters from target to source          -- [B, C=2, 6]\n","        intrinsics: camera intrinsic matrix                       -- [B, C=2, 3, 3]\n","    Return:\n","        total photometric loss related to rigid displacement\n","    \"\"\"\n","    def one_scale(depth): # [B, 1, H, W]\n","        assert(pose.size(1) == len(src_imgs))\n","       \n","        reconstruction_loss = 0\n","\n","        # retrieve depth size\n","        b, _, h, w = depth.size()\n","        \n","        # Scale the source and target image to the size of the respective depth map\n","        tgt_img_scaled = F.interpolate(tgt_img, (h, w), mode='area') # [B, 3, H, W]\n","        src_imgs_scaled = [F.interpolate(src_img, (h, w), mode='area') for src_img in src_imgs] # [[B, 3, H, W], [B, 3, H, W]]\n","        \n","        # Scale intrinsics matrix according to scale\n","        intrinsics_scaled = scale_intrinsics(depth, tgt_img, intrinsics) #[B, C=2, 3, 3]\n","        \n","        # looping over the previous and next source image\n","        for i, src_img in enumerate(src_imgs_scaled):\n","            current_intrinsics = intrinsics_scaled[:,i] # [B, 3, 3]\n","            current_pose = pose[:,i]\n","            \n","            # warp a source image to the target image plane \n","            projected_image, valid_points = inverse_warp(tgt_img, depth[:,0], current_pose, current_intrinsics) # [B, 3, H, W], # [B, H, W]\n","\n","            # Initialization of SSIM loss\n","            ssim_loss = pytorch_ssim.SSIM(window_size = 11)\n","\n","            # calculating the valid points on the projected and original image\n","            src_img_scaled_valid = src_img * valid_points.unsqueeze(1).float() \n","            projected_image_valid = projected_image * valid_points.unsqueeze(1).float()\n","\n","            # Calculating SSIM\n","            ssim = ssim_loss(src_img_scaled_valid, projected_image_valid) \n","\n","            # Calculating the l1 loss\n","            diff = (src_img_scaled_valid - projected_image) * valid_points.unsqueeze(1).float() \n","            diff_abs = diff.abs().mean()\n","            \n","            # reconstruction_loss = diff_abs\n","            reconstruction_loss += 0.85 * ((1-ssim)/2) + (1-0.85) * diff_abs      \n","                        \n","        return reconstruction_loss\n","\n","    # ignore\n","    warped_results, diff_results = 0, 0\n","    total_loss = 0\n","\n","    # looping over the depths at each scale\n","    for i, depth in enumerate(depths):\n","        loss = one_scale(depth)                      \n","        total_loss += loss\n","\n","    return total_loss, warped_results, diff_results\n"],"execution_count":283,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jmYFZ0vg60aU","colab_type":"text"},"source":["### Epipolar loss"]},{"cell_type":"code","metadata":{"id":"h3b3d1gGEDiR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593630532572,"user_tz":-120,"elapsed":1183,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["\n","########################## Epipolar loss ######################################\n","\n","def epipolar_loss(intrinsics, pose, src_imgs, flows):\n","    \"\"\" Calculate the epipoplar constraint loss \n","    Args:\n","        intrinsics: camera intrinsic matrix                       -- [B, C=2, 3, 3]\n","        pose: 6DoF pose parameters from target to source          -- [B, 6]\n","        src_imgs: list of the source images (previous & next)     -- [[B, 3, H, W], [B, 3, H, W]]\n","        flows: flow maps -- [[B, 2, H, W]....8x] \n","    Returns:\n","        Epipolar_loss: mean epipolar loss averaged over all pixels and batch - [1,1]\n","    \"\"\"\n","\n","    def one_scale_flow(flow_local, nr_src_img): # [B, 1, H, W]\n","        \"\"\" Calculate the mean of the pixelwise epipolar loss for one scale and summed over the entire batch\n","        Args:\n","            flow_local: flowmap at 1 scale - [B, 2, H, W]\n","            nr_src_img: index indicating the source image to be evaluated - (prev=0, next=1)\n","        Return:\n","            loss_one_scale: total epipolar loss for one scale and summed over the entire batch. Mean of epipolar loss over all pixels is taken. [1,1]\n","        \"\"\"\n","        # retrieve depth size and downscale factor\n","        batch, _, h, w = flow_local.size() \n","        downscale = src_imgs[0].size(2)/h\n","\n","        # Scale the source image to the size of the respective flow map\n","        src_imgs_scaled = [F.interpolate(src_img, (h, w), mode='area') for src_img in src_imgs] # [[B, 3, H, W], [B, 3, H, W]]\n","\n","        # define downscaling matrix\n","        downscale_matrix = torch.tensor([[1, 1, 1/downscale],\n","                        [1, 1, 1/downscale],\n","                        [1, 1, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n","\n","        # Scale intrinsics matrix according to scale\n","        intrinsics_scaled = intrinsics * downscale_matrix #[B, C=2, 3, 3]\n","\n","        # Determine the current intrinsics and pose\n","        current_intrinsics = intrinsics_scaled[:, nr_src_img] # [B, 3, 3]\n","        current_pose = pose[:, nr_src_img]\n","\n","        # Calculate transformation matrix from current pose\n","        transform = pose_vec2mat(current_pose) # [B, 3, 4]\n","        \n","        # extract rotation matrix and translation vector from transformation matrix of pose c\n","        rotation_mat, translation_vec = transform[:, :, :3], transform[:, :, -1:] # [B, 3, 3], [B, 3, 1]\n","\n","        # convert translation vector of pose c to skew symmetric matrix\n","        translation_mat = vec2skew_symmetric(translation_vec.squeeze(2))\n","\n","        # Obtain transposed and inverse intrinsics matrix\n","        tr_intrinsics, inv_intrinsics = current_intrinsics.permute(0, 2, 1), torch.inverse(current_intrinsics) # [B, 3, 3]  \n","        \n","        # Obtain homogeneous coordinates in pixel coordinate system and prepare for multiplications\n","        pixel_coords_src = make_grid(flow_local).reshape(1, 3, -1).permute(2, 0, 1) # [H*W, 1, 3]\n","        \n","        # Calculate the constant component of the loss function \n","        const_component = tr_intrinsics @ rotation_mat @ translation_mat @ inv_intrinsics # [B, 3, 3]\n","\n","        # obtain the target image pixels that are related to source image pixels via optical flow and make homogeneous\n","        pixel_coords_tgt, valid_points = optical_flow_displacement(flow_local, src_imgs_scaled[nr_src_img]) # [B, 2, H, W], [B, H, W]\n","        pixel_coords_tgt = pixel_coords_tgt.reshape(batch, 2, -1).permute(2,1,0) # [H*W, 2, B]\n","\n","        ones = torch.ones(h*w,1,batch).type_as(pixel_coords_tgt) # [H*W, 1, B]\n","        pixel_coords_tgt = torch.cat((pixel_coords_tgt, ones), dim=1)  # [H*W, 3, B]\n","\n","        loss_batch = 0.0\n","        # Calculate the epipolar loss for each pixel on one scale for the entire batch. \n","        #Final result per iteration over 'one_scale_flow' = [[H*W/downscale, 1, 1] ... x B * nr_src_imgs]\n","        for b in range(batch):\n","            loss_img = pixel_coords_src @ const_component[b].unsqueeze(0) @ pixel_coords_tgt[:, :, b].unsqueeze(2) # [H*W/downscale , 1, 1] \n","            loss_batch += loss_img # total loss for each pixel on 1 scale\n","\n","        # total epipolar loss for one scale and summed over the entire batch. Mean of epipolar loss over all pixels is taken\n","        loss_one_scale = loss_batch.mean()\n","\n","        return loss_one_scale\n","\n","    epipolar_loss = 0\n","    for idx, flow in enumerate(flows):\n","        if idx < 4:\n","            nr_src_img = 0\n","        else:\n","            nr_src_img = 1\n","        loss = one_scale_flow(flow, nr_src_img)\n","\n","        #  sum loss on each scale for both src images to get total epipolar loss per batch\n","        epipolar_loss += loss\n","\n","    return epipolar_loss       \n","\n"],"execution_count":272,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RwnvW-JP62et","colab_type":"text"},"source":["### Multi-View 3D Consistency loss"]},{"cell_type":"code","metadata":{"id":"9SlUtSrR64uc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593630535267,"user_tz":-120,"elapsed":1477,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["def mvs_loss(intrinsics, pose, src_imgs, tgt_img, depths_src, depths_tgt):\n","    \"\"\" Calculate \n","    Args:\n","        pose: 6DoF pose parameters from target to source            -- [B, C=2, 6]\n","        intrinsics: camera intrinsic matrix                         -- [B, C=2, 3, 3]\n","        src_imgs: list of the source images (previous & next)       -- [[B, 3, H, W], ... x2]\n","        tgt_img: target image                                       -- [B, 3, H, W]\n","        depths_src: list of depth maps of source images on 4 scales -- [[B, 1, H, W], .... x8] \n","        depths_tgt: list of depth maps of target image on 4 scales  -- [[B, 1, H, W], .... x4]\n","    Return:\n","        mvs_loss: multi-view 3D structure consistency loss\n","    \"\"\"\n","\n","    def one_scale_mvs(local_depth_tgt, local_depth_src, nr_src_img):\n","        \"\"\" Calculate the L1 loss between target image and 1 source image on 1 scale\n","            local_depth_tgt: depth map of target image on one scale - [B, 1, H, W]\n","            local_depth_src: depth map of source image on one scale - [B, 1, H, W]\n","            nr_src_img: either 0 for previous src or 1 for next image\n","\n","        \"\"\"\n","        # retrieve depth size and downscale factor\n","        b, _, h, w = local_depth_tgt.size() \n","        downscale = src_imgs[0].size(2)/h\n","\n","        # Scale the source image to the size of the respective flow map\n","        src_imgs_scaled = [F.interpolate(src_img, (h, w), mode='area') for src_img in src_imgs] # [[B, 3, H, W], [B, 3, H, W]]\n","\n","        # define downscaling matrix\n","        downscale_matrix = torch.tensor([[1, 1, 1/downscale],\n","                        [1, 1, 1/downscale],\n","                        [1, 1, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n","\n","        # Scale intrinsics matrix according to scale\n","        intrinsics_scaled = intrinsics * downscale_matrix #[B, C=2, 3, 3]\n","\n","        # Determine the current intrinsics and pose. Invert the intrinsics matrix\n","        current_pose = pose[:, nr_src_img]\n","        current_intrinsics = intrinsics_scaled[:, nr_src_img] # [B, 3, 3]\n","        inv_intrinsics = torch.inverse(current_intrinsics) # [B, 3, 3] \n","\n","        # Calculate transformation matrix from current pose and make [B, 4, 4]\n","        transform = pose_vec2mat(current_pose) # [B, 3, 4]\n","        extra_row = (torch.tensor([0, 0, 0, 1]).type_as(transform)).unsqueeze(0).unsqueeze(0).repeat(b,1,1)\n","        transform_4d = torch.cat((transform, extra_row), dim=1) # [B, 4, 4]\n","        \n","        # backprojection to 3D scene position from pixel to target and source camera frame respectively: \n","        tgt_coords_tgt_frame = pixel2cam_v2(local_depth_tgt, inv_intrinsics) # [B, 3, H, W] depth * K^-1 * p'\n","        src_coords_src_frame = pixel2cam_v2(local_depth_src, inv_intrinsics) # [B, 3, H, W] depth* K^-1 * p\n","        \n","         # Obtain homogeneous coordinates in pixel coordinate system and prepare for multiplications with M\n","        ones = torch.ones(b, 1, h, w).type_as(src_coords_src_frame) # [B, 1, H, W]\n","\n","        tgt_coords_tgt_frame_4d = torch.cat((tgt_coords_tgt_frame, ones), dim=1) # [B, 4, H, W]\n","        src_coords_src_frame_4d = torch.cat((src_coords_src_frame, ones), dim=1) # [B, 4, H, W]\n","        src_coords_src_frame_reshape = src_coords_src_frame_4d.reshape(b, 4, -1).permute(0, 2, 1).unsqueeze(3) # [B, H*W, 4, 1]\n","\n","        # multiplication with transformation matrix to backproject source pixel into target camera coordinate system\n","        src_coords_tgt_frame = transform_4d.unsqueeze(1) @ src_coords_src_frame_reshape# [B, H*W, 4]\n","        src_coords_tgt_frame = src_coords_tgt_frame.squeeze(3).permute(0, 2, 1).reshape(b, 4, h, w)\n","\n","        # L1 loss\n","        l1loss = nn.L1Loss()\n","        loss = l1loss(tgt_coords_tgt_frame_4d, src_coords_tgt_frame)\n","\n","        return loss\n","\n","    batch_mvs_loss = 0\n","    # loop over the number of source images\n","    for nr_src_img in range(len(src_imgs)):\n","        # loop over the depth scales of the target image\n","        for scale, depth_tgt in enumerate(depths_tgt):\n","            if nr_src_img == 0:\n","                loss = one_scale_mvs(depth_tgt, depths_src[scale], nr_src_img)\n","            else:\n","                loss = one_scale_mvs(depth_tgt, depths_src[scale+(len(depths_tgt))], nr_src_img)\n","            batch_mvs_loss += loss\n","    \n","    return batch_mvs_loss\n","\n","\n"],"execution_count":273,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKQrijGc8ese","colab_type":"text"},"source":["### Training and validation"]},{"cell_type":"code","metadata":{"id":"XIkoKA5qOVqk","colab_type":"code","colab":{}},"source":["def validate_with_gt_during_training(test_loader, disp_net, abs_diff, abs_rel, sq_rel, rmse, rmse_log):\n","    # Set networks into evaluation mode\n","    disp_net.eval(), pose_net.eval(), flow_net.eval()\n","\n","    # evaluate the network\n","    errors = validate_with_gt(test_loader, disp_net)\n","\n","    # reset models to training\n","    disp_net.train(), pose_net.train(), flow_net.train()\n","\n","    abs_diff.append(errors[0]), abs_rel.append(errors[1]), sq_rel.append(errors[2])\n","    rmse.append(errors[3]), rmse_log.append(errors[4])\n","    return abs_diff, abs_rel, sq_rel, rmse, rmse_log"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HzoWatpcnLQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593636389632,"user_tz":-120,"elapsed":1264,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["############################## Training One Epoch #################################\n","def standard_train(disp_net, pose_net, photometric_loss, epipolar_loss, mvs_loss, optimizer, n_epochs, train_loader, val_loader, test_loader):\n","    abs_diff, abs_rel, sq_rel, rmse, rmse_log = [], [], [], [], []   \n","    loss_train, loss_val, b100loss_list = [], [], []        \n","    # loop over the total number of epochs\n","    for epoch in range(n_epochs):\n","        running_loss = 0\n","        running_loss_val = 0\n","        b100loss = 0\n","\n","        for phase in ['train', 'val']:\n","\n","            # set the network architectures to training mode\n","            if phase == 'train':\n","                disp_net.train()\n","                pose_net.train()\n","                flow_net.train()\n","\n","            else:\n","                disp_net.eval()\n","                pose_net.eval()\n","                flow_net.eval()\n","\n","            # loop through the batches\n","            for i, data in enumerate(data_loader[phase]):\n","                \n","                # extract source and target image for 1 forward pass. Send to GPU\n","                tgt_img = data['target_image'].to(device)\n","                src_img_prev = data['source_image_prev'].to(device) \n","                src_img_next = data['source_image_next'].to(device) \n","                # Concatenate source images, and concatenate source and target image\n","                src_images = [src_img_prev , src_img_next]\n","                concat_tgt_src = [torch.cat((tgt_img, src_img), 1) for src_img in src_images] # [[B, 6, H, W], .. (2x)] \n","                     \n","                with torch.set_grad_enabled(phase == 'train'): # tracking history; only during training\n","\n","                    ############################# DEPTH #############################\n","\n","                    # predict disparity of target image and translate to depth at four scales\n","                    disparities_tgt = disp_net(tgt_img)\n","                      \n","                    # predict disparities of source images and translate to depth at four scales for each src image\n","                    disparities_src_prev = disp_net(src_img_prev)\n","                    disparities_src_next = disp_net(src_img_next)\n","                    disparities_src = [*disparities_src_prev, *disparities_src_next]\n","\n","                    # convert disparity to depth\n","                    if phase == 'train':\n","                        depths_tgt = [1/disp for disp in disparities_tgt] # [[B, 1, H, W], ... (4x) ]   \n","                        depths_src = [1/disp for disp in disparities_src] # [[B, 1, H, W], ... (8x) ]\n","                    else:                    \n","                        depths_tgt = [(1/disp).unsqueeze(1) for disp in disparities_tgt] # [[B, 1, H, W], ... (4x) ]\n","                        depths_src = [(1/disp).unsqueeze(1) for disp in disparities_src] # [[B, 1, H, W], ... (8x) ]\n","\n","                    #############################  POSE  ##############################\n","                    # predict pose focal lengths \n","                    pose, focal_lengths = pose_net(tgt_img, src_images)  # [B, C=2, 6] and [B, C=2, 2]\n","                            \n","                    # calculate the camera intrinsics matrix\n","                    intrinsics = torch.stack([focal2intrinsics(focal_lengths[:,c,:], tgt_img) for c in range(len(src_images))], dim=1) #[B, C=2, 3, 3]                \n","\n","                    ############################## FLOW ################################\n","\n","                    # predict flow map between the two source images and target image. Put in a list\n","                    flow = [*flow_net(concat_tgt_src[0]), *flow_net(concat_tgt_src[1])] # [[B, 2, H, W], ...x8]\n","\n","                    ########################## LOSS AND BACKPROP ###########################\n","                    loss_pc, warped, diff = photometric_loss(tgt_img, src_images, depths_src, flow, pose, intrinsics) ### ADD 'flow' BETWEEN DEPTHS AND POSE FOR APC \n","                    loss_e = epipolar_loss(intrinsics, pose, src_images, flow)                               ### comment this loss away\n","                    loss_mvs = mvs_loss(intrinsics, pose, src_images, tgt_img, depths_src, depths_tgt)\n","                    loss = loss_pc + 0.1 * loss_mvs + 0.001 * loss_e\n","\n","                    # In training mode; zero the gradients, backprop and optimization of parameters\n","                    if phase == 'train':\n","                        optimizer.zero_grad()\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # update intermediate batch 100 loss and running loss\n","                b100loss += loss\n","                running_loss += loss\n","\n","                # In training mode; print intermediate results every 100 batches\n","                if phase == 'train' and i % 100 == 0 and i != 0:\n","                    avg_100_batch_loss = float(b100loss / 100)\n","                    b100loss_list.append(avg_100_batch_loss)\n","                    print('batch: ', i), print('100 batch avg loss = ', avg_100_batch_loss)   \n","                    print('loss e =', loss_e), print('loss mvs =', loss_mvs), print('loss ap =', loss_pc)\n","\n","                    # Save the results and plot the training curve \n","                    save_images_norm(output_directory_train, depths_tgt, tgt_img, i)\n","                    plot_train_curve(b100loss_list)\n","\n","                    b100loss = 0\n","\n","                    ############## VALIDATE EVERY 100 BATCHES #####################\n","                    abs_diff, abs_rel, sq_rel, rmse, rmse_log = validate_with_gt_during_training(test_loader, disp_net, abs_diff, abs_rel, sq_rel, rmse, rmse_log)  \n","                    plot_metrics(abs_diff, abs_rel, sq_rel, rmse, rmse_log)\n","\n","                    save_models(disp_net, pose_net, flow_net, model_path_disp, epoch)\n","                    print('Model_saved')\n","\n","        ######################## CALCULATE LOSSES ######################\n","        # Calculate training van validation loss and plot the resulting curves\n","        running_loss_avg = running_loss/(len(train_loader.dataset)/4)\n","        running_loss_val_avg = running_loss_val/len(val_loader.dataset)\n","        loss_train.append(running_loss_avg)\n","        loss_val.append(running_loss_val_avg)\n","        plot_curve(loss_train, loss_val)\n","\n","        print('epoch:', epoch)\n","        print('Training Loss: {:.4f}'.format(running_loss/(len(data_loader['train'].dataset)/4)))\n","        print('Validation Loss: {:.4f}'.format(running_loss_val/(len(data_loader['val'].dataset)/1)))\n","        "],"execution_count":298,"outputs":[]},{"cell_type":"code","metadata":{"id":"7UsS--75byVB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593630541191,"user_tz":-120,"elapsed":1222,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["def compute_errors(gt, pred, crop=True):\n","    abs_diff, abs_rel, sq_rel, a1, a2, a3 = 0,0,0,0,0,0\n","    batch_size = gt.size(0)\n","\n","    '''\n","    crop used by Garg ECCV16 to reprocude Eigen NIPS14 results\n","    construct a mask of False values, with the same size as target\n","    and then set to True values inside the crop\n","    '''\n","    if crop:\n","        crop_mask = gt[0] != gt[0]\n","        y1,y2 = int(0.40810811 * gt.size(1)), int(0.99189189 * gt.size(1))\n","        x1,x2 = int(0.03594771 * gt.size(2)), int(0.96405229 * gt.size(2))\n","        crop_mask[y1:y2,x1:x2] = 1\n","\n","    for current_gt, current_pred in zip(gt, pred):\n","        valid = (current_gt > 0) & (current_gt < 80)\n","        if crop:\n","            valid = valid & crop_mask\n","\n","        valid_gt = current_gt[valid]\n","        valid_pred = current_pred[valid].clamp(1e-3, 80)\n","\n","        valid_pred = valid_pred * torch.median(valid_gt)/torch.median(valid_pred)\n","\n","        thresh = torch.max((valid_gt / valid_pred), (valid_pred / valid_gt))\n","        a1 += (thresh < 1.25).float().mean()\n","        a2 += (thresh < 1.25 ** 2).float().mean()\n","        a3 += (thresh < 1.25 ** 3).float().mean()\n","\n","        rmse = (valid_gt - valid_pred) ** 2\n","        rmse = torch.sqrt(rmse.mean())\n","\n","        rmse_log = (torch.log(valid_gt) - torch.log(valid_pred)) ** 2\n","        rmse_log = torch.sqrt(rmse_log.mean())\n","\n","        abs_diff += torch.mean(torch.abs(valid_gt - valid_pred))\n","        abs_rel += torch.mean(torch.abs(valid_gt - valid_pred) / valid_gt)\n","\n","        sq_rel += torch.mean(((valid_gt - valid_pred)**2) / valid_gt)\n","\n","    return [metric.item() / batch_size for metric in [abs_diff, abs_rel, sq_rel, rmse, rmse_log, a3]]"],"execution_count":276,"outputs":[]},{"cell_type":"code","metadata":{"id":"2olVj5DjNflV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593630542901,"user_tz":-120,"elapsed":1143,"user":{"displayName":"Seger Tak","photoUrl":"","userId":"06326321139604153584"}}},"source":["def validate_with_gt(val_loader, disp_net):\n","        # intialize validation metrics             \n","        #abs_diff_sum, abs_rel_sum, sq_rel_sum, a1_sum, a2_sum, a3_sum = 0,0,0,0,0,0\n","        #n = len(val_loader)\n","\n","        disp_net.eval()\n","        pose_net.eval()\n","        flow_net.eval()\n","        errors = []\n","\n","        with torch.no_grad():\n","\n","            for (i, data) in enumerate(val_loader):\n","\n","                tgt_image = data['image'].to(device)\n","                gt_depth = data['gt_depth'].to(device)\n","\n","                # 1 forward pass in eval mode\n","                output_disp = disp_net(tgt_image)\n","\n","                # inverse disparity to obtain depth\n","                output_depth = 1/output_disp[:,0]\n","\n","                if i % 25 == 0:\n","                    depth_to_img = skimage.transform.resize(output_depth.squeeze().cpu().detach(), [375, 1242], mode='constant')\n","                    gt_to_img = skimage.transform.resize(gt_depth.squeeze().cpu().detach(), [375, 1242], mode='constant')\n","                    plt.imsave(os.path.join(output_directory_eval, str(i)+ '_L1_pred.png'), depth_to_img, cmap='plasma')\n","                    plt.imsave(os.path.join(output_directory_eval, str(i)+'_L1_gt.png'), gt_to_img, cmap='plasma')              \n","                \n","                errors.append(compute_errors(gt_depth, output_depth, crop=False))\n","\n","        mean_errors = np.array(errors).mean(0)\n","\n","        print(\"\\n  \" + (\"{:>8} | \" * 6).format(\"abs_diff\", \"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a3\"))\n","        print((\"&{: 8.3f}  \" * 6).format(*mean_errors.tolist()) + \"\\\\\\\\\")\n","        return mean_errors"],"execution_count":277,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"umYdxoVKpcZ0","colab_type":"text"},"source":["## Start training"]},{"cell_type":"code","metadata":{"id":"CC330OYVBgdn","colab_type":"code","colab":{}},"source":["standard_train(disp_net, pose_net, adaptive_photometric_loss, epipolar_loss, mvs_loss, optimizer, 5, train_loader, val_loader, test_loader)"],"execution_count":null,"outputs":[]}]}